{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TRANSFORMERS FROM SCRATCH.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQohGWSTyT3Ih/6GRfVwl5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlirezaSaberi/former/blob/master/TRANSFORMERS_FROM_SCRATCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl7olt57nl5K",
        "colab_type": "text"
      },
      "source": [
        "[**TRANSFORMERS FROM SCRATCH**](http://peterbloem.nl/blog/transformers)\n",
        "\n",
        "Blog: http://peterbloem.nl/blog/transformers\n",
        "\n",
        "Gitbub repo: https://github.com/pbloem/former"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jyg_fiYt4ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch, os\n",
        "## util.py\n",
        "def mask_(matrices, maskval=0.0, mask_diagonal=True):\n",
        "    \"\"\"\n",
        "    Masks out all values in the given batch of matrices where i <= j holds,\n",
        "    i < j if mask_diagonal is false\n",
        "    In place operation\n",
        "    :param tns:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    b, h, w = matrices.size()\n",
        "\n",
        "    indices = torch.triu_indices(h, w, offset=0 if mask_diagonal else 1)\n",
        "    matrices[:, indices[0], indices[1]] = maskval\n",
        "\n",
        "def d(tensor=None):\n",
        "    \"\"\"\n",
        "    Returns a device string either for the best available device,\n",
        "    or for the device corresponding to the argument\n",
        "    :param tensor:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if tensor is None:\n",
        "        return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    return 'cuda' if tensor.is_cuda else 'cpu'\n",
        "\n",
        "def here(subpath=None):\n",
        "    \"\"\"\n",
        "    :return: the path in which the package resides (the directory containing the 'former' dir)\n",
        "    \"\"\"\n",
        "    if subpath is None:\n",
        "        return os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))\n",
        "\n",
        "    return os.path.abspath(os.path.join(os.path.dirname(__file__), '../..', subpath))\n",
        "\n",
        "def contains_nan(tensor):\n",
        "    return bool((tensor != tensor).sum() > 0)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcp0LGmPt6HI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## modules\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import random, math\n",
        "\n",
        "class SelfAttentionWide(nn.Module):\n",
        "    def __init__(self, emb, heads=8, mask=False):\n",
        "        \"\"\"\n",
        "        :param emb:\n",
        "        :param heads:\n",
        "        :param mask:\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb = emb\n",
        "        self.heads = heads\n",
        "        self.mask = mask\n",
        "\n",
        "        self.tokeys = nn.Linear(emb, emb * heads, bias=False)\n",
        "        self.toqueries = nn.Linear(emb, emb * heads, bias=False)\n",
        "        self.tovalues = nn.Linear(emb, emb * heads, bias=False)\n",
        "\n",
        "        self.unifyheads = nn.Linear(heads * emb, emb)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        b, t, e = x.size()\n",
        "        h = self.heads\n",
        "        assert e == self.emb, f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'\n",
        "\n",
        "        keys    = self.tokeys(x)   .view(b, t, h, e)\n",
        "        queries = self.toqueries(x).view(b, t, h, e)\n",
        "        values  = self.tovalues(x) .view(b, t, h, e)\n",
        "\n",
        "        # compute scaled dot-product self-attention\n",
        "\n",
        "        # - fold heads into the batch dimension\n",
        "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, e)\n",
        "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, e)\n",
        "        values = values.transpose(1, 2).contiguous().view(b * h, t, e)\n",
        "\n",
        "        queries = queries / (e ** (1/4))\n",
        "        keys    = keys / (e ** (1/4))\n",
        "        # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
        "        #   This should be more memory efficient\n",
        "\n",
        "        # - get dot product of queries and keys, and scale\n",
        "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
        "\n",
        "        assert dot.size() == (b*h, t, t)\n",
        "\n",
        "        if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n",
        "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
        "\n",
        "        dot = F.softmax(dot, dim=2)\n",
        "        # - dot now has row-wise self-attention probabilities\n",
        "\n",
        "        # apply the self attention to the values\n",
        "        out = torch.bmm(dot, values).view(b, h, t, e)\n",
        "\n",
        "        # swap h, t back, unify heads\n",
        "        out = out.transpose(1, 2).contiguous().view(b, t, h * e)\n",
        "\n",
        "        return self.unifyheads(out)\n",
        "\n",
        "class SelfAttentionNarrow(nn.Module):\n",
        "\n",
        "    def __init__(self, emb, heads=8, mask=False):\n",
        "        \"\"\"\n",
        "        :param emb:\n",
        "        :param heads:\n",
        "        :param mask:\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        assert emb % heads == 0, f'Embedding dimension ({emb}) should be divisible by nr. of heads ({heads})'\n",
        "\n",
        "        self.emb = emb\n",
        "        self.heads = heads\n",
        "        self.mask = mask\n",
        "\n",
        "        s = emb // heads\n",
        "        # - We will break the embedding into `heads` chunks and feed each to a different attention head\n",
        "\n",
        "        self.tokeys    = nn.Linear(s, s, bias=False)\n",
        "        self.toqueries = nn.Linear(s, s, bias=False)\n",
        "        self.tovalues  = nn.Linear(s, s, bias=False)\n",
        "\n",
        "        self.unifyheads = nn.Linear(heads * s, emb)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        b, t, e = x.size()\n",
        "        h = self.heads\n",
        "        assert e == self.emb, f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'\n",
        "\n",
        "        s = e // h\n",
        "        x = x.view(b, t, h, s)\n",
        "\n",
        "        keys    = self.tokeys(x)\n",
        "        queries = self.toqueries(x)\n",
        "        values  = self.tovalues(x)\n",
        "\n",
        "        assert keys.size() == (b, t, h, s)\n",
        "        assert queries.size() == (b, t, h, s)\n",
        "        assert values.size() == (b, t, h, s)\n",
        "\n",
        "        # Compute scaled dot-product self-attention\n",
        "\n",
        "        # - fold heads into the batch dimension\n",
        "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
        "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
        "        values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
        "\n",
        "        queries = queries / (e ** (1/4))\n",
        "        keys    = keys / (e ** (1/4))\n",
        "        # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
        "        #   This should be more memory efficient\n",
        "\n",
        "        # - get dot product of queries and keys, and scale\n",
        "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
        "\n",
        "        assert dot.size() == (b*h, t, t)\n",
        "\n",
        "        if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n",
        "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
        "\n",
        "        dot = F.softmax(dot, dim=2)\n",
        "        # - dot now has row-wise self-attention probabilities\n",
        "\n",
        "        # apply the self attention to the values\n",
        "        out = torch.bmm(dot, values).view(b, h, t, s)\n",
        "\n",
        "        # swap h, t back, unify heads\n",
        "        out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n",
        "\n",
        "        return self.unifyheads(out)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, emb, heads, mask, seq_length, ff_hidden_mult=4, dropout=0.0, wide=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = SelfAttentionWide(emb, heads=heads, mask=mask) if wide \\\n",
        "                    else SelfAttentionNarrow(emb, heads=heads, mask=mask)\n",
        "        self.mask = mask\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(emb)\n",
        "        self.norm2 = nn.LayerNorm(emb)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(emb, ff_hidden_mult * emb),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_hidden_mult * emb, emb)\n",
        "        )\n",
        "\n",
        "        self.do = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        attended = self.attention(x)\n",
        "\n",
        "        x = self.norm1(attended + x)\n",
        "\n",
        "        x = self.do(x)\n",
        "\n",
        "        fedforward = self.ff(x)\n",
        "\n",
        "        x = self.norm2(fedforward + x)\n",
        "\n",
        "        x = self.do(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWlsIfP2uPy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## tranformer\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer for generating text (character by character).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emb, heads, depth, seq_length, num_tokens, wide=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_tokens = num_tokens\n",
        "        self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n",
        "        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)\n",
        "\n",
        "        tblocks = []\n",
        "        for i in range(depth):\n",
        "            tblocks.append(\n",
        "                TransformerBlock(emb=emb, heads=heads, seq_length=seq_length, mask=True, wide=wide))\n",
        "\n",
        "        self.tblocks = nn.Sequential(*tblocks)\n",
        "\n",
        "        self.toprobs = nn.Linear(emb, num_tokens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: A (batch, sequence length) integer tensor of token indices.\n",
        "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
        "        \"\"\"\n",
        "        tokens = self.token_embedding(x)\n",
        "        b, t, e = tokens.size()\n",
        "\n",
        "        positions = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b, t, e)\n",
        "        x = tokens + positions\n",
        "\n",
        "        x = self.tblocks(x)\n",
        "\n",
        "        x = self.toprobs(x.view(b*t, e)).view(b, t, self.num_tokens)\n",
        "\n",
        "        return F.log_softmax(x, dim=2)\n",
        "\n",
        "class CTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer for classifying sequences\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emb, heads, depth, seq_length, num_tokens, num_classes, max_pool=True, dropout=0.0, wide=False):\n",
        "        \"\"\"\n",
        "        :param emb: Embedding dimension\n",
        "        :param heads: nr. of attention heads\n",
        "        :param depth: Number of transformer blocks\n",
        "        :param seq_length: Expected maximum sequence length\n",
        "        :param num_tokens: Number of tokens (usually words) in the vocabulary\n",
        "        :param num_classes: Number of classes.\n",
        "        :param max_pool: If true, use global max pooling in the last layer. If false, use global\n",
        "                         average pooling.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_tokens, self.max_pool = num_tokens, max_pool\n",
        "\n",
        "        self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n",
        "        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)\n",
        "\n",
        "        tblocks = []\n",
        "        for i in range(depth):\n",
        "            tblocks.append(\n",
        "                TransformerBlock(emb=emb, heads=heads, seq_length=seq_length, mask=False, dropout=dropout, wide=wide))\n",
        "\n",
        "        self.tblocks = nn.Sequential(*tblocks)\n",
        "\n",
        "        self.toprobs = nn.Linear(emb, num_classes)\n",
        "\n",
        "        self.do = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: A batch by sequence length integer tensor of token indices.\n",
        "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
        "        \"\"\"\n",
        "        tokens = self.token_embedding(x)\n",
        "        b, t, e = tokens.size()\n",
        "\n",
        "        positions = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b, t, e)\n",
        "        x = tokens + positions\n",
        "        x = self.do(x)\n",
        "\n",
        "        x = self.tblocks(x)\n",
        "\n",
        "        x = x.max(dim=1)[0] if self.max_pool else x.mean(dim=1) # pool over the time dimension\n",
        "\n",
        "        x = self.toprobs(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vQB1CPAuakQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext import data, datasets, vocab\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from argparse import ArgumentParser\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import random, tqdm, sys, math, gzip\n",
        "\n",
        "# Used for converting between nats and bits\n",
        "LOG2E = math.log2(math.e)\n",
        "TEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\n",
        "LABEL = data.Field(sequential=False)\n",
        "NUM_CLS = 2\n",
        "\n",
        "def go(arg):\n",
        "    \"\"\"\n",
        "    Creates and trains a basic transformer for the IMDB sentiment classification task.\n",
        "    \"\"\"\n",
        "    tbw = SummaryWriter(log_dir=arg.tb_dir) # Tensorboard logging\n",
        "\n",
        "    # load the IMDB data\n",
        "    if arg.final:\n",
        "        train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "        TEXT.build_vocab(train, max_size=arg.vocab_size - 2)\n",
        "        LABEL.build_vocab(train)\n",
        "\n",
        "        train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=arg.batch_size, device=d())\n",
        "    else:\n",
        "        tdata, _ = datasets.IMDB.splits(TEXT, LABEL)\n",
        "        train, test = tdata.split(split_ratio=0.8)\n",
        "\n",
        "        TEXT.build_vocab(train, max_size=arg.vocab_size - 2) # - 2 to make space for <unk> and <pad>\n",
        "        LABEL.build_vocab(train)\n",
        "\n",
        "        train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=arg.batch_size, device=d())\n",
        "\n",
        "    print(f'- nr. of training examples {len(train_iter)}')\n",
        "    print(f'- nr. of {\"test\" if arg.final else \"validation\"} examples {len(test_iter)}')\n",
        "\n",
        "    if arg.max_length < 0:\n",
        "        mx = max([input.text[0].size(1) for input in train_iter])\n",
        "        mx = mx * 2\n",
        "        print(f'- maximum sequence length: {mx}')\n",
        "    else:\n",
        "        mx = arg.max_length\n",
        "\n",
        "    # create the model\n",
        "    model = CTransformer(emb=arg.embedding_size, heads=arg.num_heads, depth=arg.depth, seq_length=mx, num_tokens=arg.vocab_size, num_classes=NUM_CLS, max_pool=arg.max_pool)\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "\n",
        "    opt = torch.optim.Adam(lr=arg.lr, params=model.parameters())\n",
        "    sch = torch.optim.lr_scheduler.LambdaLR(opt, lambda i: min(i / (arg.lr_warmup / arg.batch_size), 1.0))\n",
        "\n",
        "    # training loop\n",
        "    seen = 0\n",
        "    for e in range(arg.num_epochs):\n",
        "\n",
        "        print(f'\\n epoch {e}')\n",
        "        model.train(True)\n",
        "\n",
        "        for batch in tqdm.tqdm(train_iter):\n",
        "\n",
        "            opt.zero_grad()\n",
        "\n",
        "            input = batch.text[0]\n",
        "            label = batch.label - 1\n",
        "\n",
        "            if input.size(1) > mx:\n",
        "                input = input[:, :mx]\n",
        "            out = model(input)\n",
        "            loss = F.nll_loss(out, label)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # clip gradients\n",
        "            # - If the total gradient vector has a length > 1, we clip it back down to 1.\n",
        "            if arg.gradient_clipping > 0.0:\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), arg.gradient_clipping)\n",
        "\n",
        "            opt.step()\n",
        "            sch.step()\n",
        "\n",
        "            seen += input.size(0)\n",
        "            tbw.add_scalar('classification/train-loss', float(loss.item()), seen)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            model.train(False)\n",
        "            tot, cor= 0.0, 0.0\n",
        "\n",
        "            for batch in test_iter:\n",
        "\n",
        "                input = batch.text[0]\n",
        "                label = batch.label - 1\n",
        "\n",
        "                if input.size(1) > mx:\n",
        "                    input = input[:, :mx]\n",
        "                out = model(input).argmax(dim=1)\n",
        "\n",
        "                tot += float(input.size(0))\n",
        "                cor += float((label == out).sum().item())\n",
        "\n",
        "            acc = cor / tot\n",
        "            print(f'-- {\"test\" if arg.final else \"validation\"} accuracy {acc:.3}')\n",
        "            tbw.add_scalar('classification/test-loss', float(loss.item()), e)\n",
        "\n",
        "\n",
        "def do_classification(args=[]):\n",
        "\n",
        "    parser = ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"-e\", \"--num-epochs\",\n",
        "                        dest=\"num_epochs\",\n",
        "                        help=\"Number of epochs.\",\n",
        "                        default=80, type=int)\n",
        "\n",
        "    parser.add_argument(\"-b\", \"--batch-size\",\n",
        "                        dest=\"batch_size\",\n",
        "                        help=\"The batch size.\",\n",
        "                        default=4, type=int)\n",
        "\n",
        "    parser.add_argument(\"-l\", \"--learn-rate\",\n",
        "                        dest=\"lr\",\n",
        "                        help=\"Learning rate\",\n",
        "                        default=0.0001, type=float)\n",
        "\n",
        "    parser.add_argument(\"-T\", \"--tb_dir\", dest=\"tb_dir\",\n",
        "                        help=\"Tensorboard logging directory\",\n",
        "                        default='./runs')\n",
        "\n",
        "    parser.add_argument(\"-f\", \"--final\", dest=\"final\",\n",
        "                        help=\"Whether to run on the real test set (if not included, the validation set is used).\",\n",
        "                        action=\"store_true\")\n",
        "\n",
        "    parser.add_argument(\"--max-pool\", dest=\"max_pool\",\n",
        "                        help=\"Use max pooling in the final classification layer.\",\n",
        "                        action=\"store_true\")\n",
        "\n",
        "    parser.add_argument(\"-E\", \"--embedding\", dest=\"embedding_size\",\n",
        "                        help=\"Size of the character embeddings.\",\n",
        "                        default=128, type=int)\n",
        "\n",
        "    parser.add_argument(\"-V\", \"--vocab-size\", dest=\"vocab_size\",\n",
        "                        help=\"Number of words in the vocabulary.\",\n",
        "                        default=50_000, type=int)\n",
        "\n",
        "    parser.add_argument(\"-M\", \"--max\", dest=\"max_length\",\n",
        "                        help=\"Max sequence length. Longer sequences are clipped (-1 for no limit).\",\n",
        "                        default=512, type=int)\n",
        "\n",
        "    parser.add_argument(\"-H\", \"--heads\", dest=\"num_heads\",\n",
        "                        help=\"Number of attention heads.\",\n",
        "                        default=8, type=int)\n",
        "\n",
        "    parser.add_argument(\"-d\", \"--depth\", dest=\"depth\",\n",
        "                        help=\"Depth of the network (nr. of self-attention layers)\",\n",
        "                        default=6, type=int)\n",
        "\n",
        "    parser.add_argument(\"-r\", \"--random-seed\",\n",
        "                        dest=\"seed\",\n",
        "                        help=\"RNG seed. Negative for random\",\n",
        "                        default=1, type=int)\n",
        "\n",
        "    parser.add_argument(\"--lr-warmup\",\n",
        "                        dest=\"lr_warmup\",\n",
        "                        help=\"Learning rate warmup.\",\n",
        "                        default=10_000, type=int)\n",
        "\n",
        "    parser.add_argument(\"--gradient-clipping\",\n",
        "                        dest=\"gradient_clipping\",\n",
        "                        help=\"Gradient clipping.\",\n",
        "                        default=1.0, type=float)\n",
        "\n",
        "    options = parser.parse_args(args)\n",
        "\n",
        "    print('OPTIONS ', options)\n",
        "\n",
        "    go(options)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R8GthZju5o7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "42b15090-93d6-4897-9e14-4549a096c6c2"
      },
      "source": [
        "# Run it when ready\n",
        "do_classification(args=[])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OPTIONS  Namespace(batch_size=4, depth=6, embedding_size=128, final=False, gradient_clipping=1.0, lr=0.0001, lr_warmup=10000, max_length=512, max_pool=False, num_epochs=80, num_heads=8, seed=1, tb_dir='./runs', vocab_size=50000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\raclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:04<00:00, 17.2MB/s]\n",
            "  0%|          | 0/5000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- nr. of training examples 5000\n",
            "- nr. of validation examples 1250\n",
            "\n",
            " epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 10%|▉         | 491/5000 [05:58<58:49,  1.28it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmja1_rSepo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}